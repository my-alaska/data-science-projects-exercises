{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8d1c2c",
   "metadata": {},
   "source": [
    "# Easy Chat Bot\n",
    "Let's import some useful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d70eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b9296",
   "metadata": {},
   "source": [
    "The data is stored in 'intents.json' file that I'm not sure where I have found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a730b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as fp:\n",
    "    contents = json.loads(fp.read())['intents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46567a",
   "metadata": {},
   "source": [
    "Let's create a function for parsing a string into separate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452aa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('','',string.punctuation + '0123456789')\n",
    "\n",
    "# function for parsing. We remove punctuation and numbers, map to lowercase and split at spaces\n",
    "def parse(sentence):\n",
    "    return sentence.translate(translator).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceff5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set() # we'll need a set to find every word occuring in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ac76f",
   "metadata": {},
   "source": [
    "We'll prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143a1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [] # arrays of tokenized sentences for each class of input text\n",
    "class_responses = [] # responses for each class of input text\n",
    "\n",
    "for i, text_data in enumerate(contents):\n",
    "    name = text_data['tag']\n",
    "    tokenized_sentences.append([])\n",
    "    class_responses.append([])\n",
    "    \n",
    "    for sentence in text_data['patterns']:\n",
    "        tok_sentence = parse(sentence)\n",
    "        tokenized_sentences[i].append(tok_sentence)\n",
    "        \n",
    "        for word in tok_sentence:\n",
    "            unique_words.add(word)\n",
    "    \n",
    "    for sentence in text_data['responses']:\n",
    "        class_responses[i].append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3cf558",
   "metadata": {},
   "source": [
    "To create a bag of words vector we need to assign an index for each word occuring in training input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd2e4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_list = sorted(unique_words)\n",
    "indexing = defaultdict(lambda: -1)\n",
    "for i, word in enumerate(unique_word_list):\n",
    "    indexing[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e6c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for bag of words encoding\n",
    "def encode_bow(string_list,indexing,unique):\n",
    "    vector = np.zeros(len(indexing.keys()))\n",
    "    for word in string_list:\n",
    "        if word in unique:\n",
    "            vector[indexing[word]] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa102d89",
   "metadata": {},
   "source": [
    "Now we only need to vectorize the sentences ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510fed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_encoded = []\n",
    "X,Y = [],[]\n",
    "\n",
    "for i, sentences in enumerate(tokenized_sentences):\n",
    "    y_vector = np.zeros(len(class_responses))\n",
    "    y_vector[i] = 1\n",
    "    for sentence in sentences:\n",
    "        X.append(encode_bow(sentence,indexing,unique_words))\n",
    "        Y.append(y_vector)\n",
    "        \n",
    "order = np.random.permutation(len(X))\n",
    "\n",
    "X,Y = np.array(X)[order],np.array(Y)[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb5925c",
   "metadata": {},
   "source": [
    "... initialize a model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c34d0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(200, input_shape=(len(X[0]),), activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(len(Y[0]), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(momentum=0.6,decay=10**-6,nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34686ca9",
   "metadata": {},
   "source": [
    "... and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f632aeb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a73d1cac0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=300, batch_size=5,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315001c8",
   "metadata": {},
   "source": [
    "### Output\n",
    "To choose a response we first need to categorize user's input. Our model will classify the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f9c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predict(sentence, model, indexing, unique_words):\n",
    "    vector = encode_bow(parse(sentence),indexing, unique_words)\n",
    "    prediction = model.predict(vector.reshape(1,-1)).squeeze()\n",
    "    if prediction.max() < 0.2:\n",
    "        return None\n",
    "    else: return prediction.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04b75e",
   "metadata": {},
   "source": [
    "We'll be choosing a random sentence from responses for each input class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b1cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(class_index, class_responses):\n",
    "    responses = class_responses[class_index]\n",
    "    n = len(responses)\n",
    "    return responses[randint(0,n-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee281b",
   "metadata": {},
   "source": [
    "Let's see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faaa81da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n",
      "Hi there, how can I help?\n"
     ]
    }
   ],
   "source": [
    "v = class_predict(\"hello there\",model,indexing,unique_words)\n",
    "print(get_response(v,class_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b669275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies\n"
     ]
    }
   ],
   "source": [
    "v2 = class_predict(\"can you help me out please?\",model,indexing,unique_words)\n",
    "print(get_response(v2,class_responses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a9eb315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Any time!\n"
     ]
    }
   ],
   "source": [
    "v3 = class_predict(\"Okay thank you I'll need to develop you further in the future\",model,indexing,unique_words)\n",
    "print(get_response(v3,class_responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fd95c",
   "metadata": {},
   "source": [
    "Unsurprisingly such small dataset won't allow us to create any complex chatbot. Moreover the bag of words methode isn't perfect(for example it doesn't allow us to include such important information as word order). I'll need to get better knowledge on natural language processing to develop this project further"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
